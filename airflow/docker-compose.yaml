# ============================================================
# docker-compose.yaml — Apache Airflow para Seminario
#
# Configuración de Airflow con:
#   - PostgreSQL dedicado como metadata backend de Airflow
#   - Scheduler + Webserver (LocalExecutor)
#   - DAGs montados desde el proyecto
#
# NOTA: El pipeline ETL principal usa su propia instancia de
#       PostgreSQL (docker-compose.yml raíz, puerto host 5433).
#       Esta instancia (puerto host 5434) es exclusiva para
#       metadatos de Airflow.
#
# Uso (desde la raíz del proyecto):
#   docker compose -f airflow/docker-compose.yaml up -d
#
#   # O con el script helper:
#   bash scripts/start_airflow.sh
#
#   # UI en http://localhost:8080
#   # Credenciales: admin / admin
#
# Nota: airflow-init se ejecuta automáticamente como dependencia.
#       No es necesario ejecutarlo por separado.
# ============================================================

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.3-python3.12
  environment: &airflow-common-env
    # ── Airflow Core ──
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__TEST_CONNECTION: Enabled
    AIRFLOW__CORE__FERNET_KEY: "${FERNET_KEY:-}"
    AIRFLOW__CORE__DEFAULT_TIMEZONE: America/Guatemala
    # ── Scheduler — Heartbeat & Timeouts ──
    # Evita que el scheduler mate tareas de cómputo intensivo (CPU-bound)
    # que no envían heartbeat a tiempo.
    # Intervalo de heartbeat del task runner (default: 5s → 30s)
    AIRFLOW__SCHEDULER__TASK_INSTANCE_HEARTBEAT_SEC: "30"
    # Multiplicador: timeout = HEARTBEAT_SEC × HEARTBEAT_TIMEOUT (30×10 = 300s = 5min)
    AIRFLOW__SCHEDULER__TASK_INSTANCE_HEARTBEAT_TIMEOUT: "10"
    # Intervalo de detección de zombies (default: 10s → 120s)
    AIRFLOW__SCHEDULER__ZOMBIE_DETECTION_INTERVAL: "120"
    # Umbral para considerar un task como zombie (default: 300s → 600s)
    AIRFLOW__SCHEDULER__LOCAL_TASK_JOB_HEARTBEAT_SEC: "30"
    # ── Metadata DB (postgres dedicado para Airflow) ──
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    # ── Webserver ──
    AIRFLOW__WEBSERVER__SECRET_KEY: "${AIRFLOW_SECRET_KEY:-temporarykey123}"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: America/Guatemala
    # ── Logging ──
    AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
    # ── Variables del proyecto Seminario (usadas por los DAGs) ──
    SEMINARIO_PROJECT_ROOT: /opt/airflow/seminario
    POSTGRES_USER: yeigen
    POSTGRES_PASSWORD: "LavidaEsbella16*#"
    POSTGRES_DB: seminario
    POSTGRES_HOST: seminario-postgres
    POSTGRES_PORT: "5432"
    # ── Dependencias adicionales del pipeline ──
    _PIP_ADDITIONAL_REQUIREMENTS: >-
      openpyxl
      google-auth
      google-auth-oauthlib
      google-api-python-client
      python-dotenv
      pandas
      psycopg2-binary
      sqlalchemy
  volumes:
    # DAGs del proyecto
    - ./dags:/opt/airflow/dags
    # Logs de Airflow
    - ./logs:/opt/airflow/logs
    # Plugins personalizados
    - ./plugins:/opt/airflow/plugins
    # Config adicional
    - ./config:/opt/airflow/config
    # ── Proyecto Seminario (read-only para los DAGs) ──
    - ../etl:/opt/airflow/seminario/etl:ro
    - ../scripts:/opt/airflow/seminario/scripts:ro
    - ../config:/opt/airflow/seminario/config:ro
    - ../utils:/opt/airflow/seminario/utils:ro
    - ../main.py:/opt/airflow/seminario/main.py:ro
    - ../pyproject.toml:/opt/airflow/seminario/pyproject.toml:ro
    # Datos del pipeline (lectura-escritura)
    - ../data:/opt/airflow/seminario/data
    - ../logs:/opt/airflow/seminario/logs
    # Token de Google Drive (read-write para permitir auto-refresh)
    - ../token.json:/opt/airflow/seminario/token.json
    # Variables de entorno del proyecto
    - ../.env:/opt/airflow/seminario/.env:ro
  user: "${AIRFLOW_UID:-50000}:0"
  restart: unless-stopped
  # DNS público para resolver oauth2.googleapis.com (refresh de tokens)
  dns:
    - 8.8.8.8
    - 8.8.4.4
  depends_on:
    airflow-postgres:
      condition: service_healthy
  networks:
    - airflow-net
    - seminario-net

services:
  # ── PostgreSQL para metadata de Airflow (dedicado) ─────────
  # Separado del postgres del pipeline ETL para evitar
  # conflictos de schemas y container_name.
  airflow-postgres:
    image: postgres:16-alpine
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5434:5432"
    volumes:
      - airflow-pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - airflow-net

  # ── Inicialización (idempotente: db migrate + user create) ─
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "══════════════════════════════════════════════════"
        echo "  Airflow Init: Migrando base de datos..."
        echo "══════════════════════════════════════════════════"
        airflow db migrate
        echo "── Creando usuario admin (si no existe) ──"
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@seminario.local 2>/dev/null || true
        echo "══════════════════════════════════════════════════"
        echo "  Airflow Init: ¡Listo!"
        echo "══════════════════════════════════════════════════"
    restart: "no"

  # ── Webserver (UI en http://localhost:8080) ─────────────────
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  # ── Scheduler ──────────────────────────────────────────────
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

# ── Redes ────────────────────────────────────────────────────
networks:
  # Red interna entre servicios de Airflow
  airflow-net:
    driver: bridge
  # Red compartida con el stack principal (seminario)
  # Permite que los DAGs de Airflow se conecten al postgres del ETL
  seminario-net:
    external: true
    name: seminario_default

# ── Volúmenes nombrados ─────────────────────────────────────
volumes:
  airflow-pgdata:
